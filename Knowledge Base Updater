from bs4 import BeautifulSoup
import requests
from sklearn.feature_extraction.text import TfidfVectorizer
import faiss

# Scrape content
url = 'https://en.wikipedia.org/wiki/Artificial_intelligence'
html = requests.get(url).text
soup = BeautifulSoup(html, 'html.parser')
text_data = ' '.join([p.text for p in soup.find_all('p')])

# Embed
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform([text_data]).toarray()

# Create FAISS index
index = faiss.IndexFlatL2(X.shape[1])
index.add(X)

# Save index
faiss.write_index(index, "knowledge_index.faiss")
